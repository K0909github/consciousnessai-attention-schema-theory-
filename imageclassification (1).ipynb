{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments 1 and 2, general and transfer image classification tasks: binary image classification tasks A, B, and C, with full training and transfer learning / frozen weights (Fig 3)"
      ],
      "metadata": {
        "id": "fh6lPei0rq-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWW51taHjaFF",
        "outputId": "4f3c099b-8ee5-434c-e925-52a648c3bde2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2RLJF3R41eFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4a475e-2396-4e49-c737-177a8ccc05f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# To run the notebook, change the string below\n",
        "dir_path = '/content/drive/MyDrive/attention_schemas_in_anns'\n",
        "\n",
        "# Uncomment to run in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# -------------------- ▼▼▼ 修正箇所 ▼▼▼ --------------------\n",
        "# sys.path を使う代わりに、importlibを使ってファイルを直接指定して読み込みます\n",
        "# これにより、他のライブラリとの名前の衝突を確実に回避します\n",
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "module_path = dir_path + \"/vision_transformer.py\"\n",
        "module_name = \"vision_transformer\"\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
        "vision_transformer = importlib.util.module_from_spec(spec)\n",
        "sys.modules[module_name] = vision_transformer\n",
        "spec.loader.exec_module(vision_transformer)\n",
        "# -------------------- ▲▲▲ ここまで ▲▲▲ --------------------\n",
        "\n",
        "\n",
        "# ViT model based on https://github.com/facebookresearch/dino/blob/main/README.md\n",
        "import torch\n",
        "import torch.random\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import fnmatch\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# IMAGE CLASSIFICATION A, B, AND C DATA LOADERS\n",
        "\n",
        "traindirA = dir_path+\"/data/train/classificationA\"\n",
        "valdirA = dir_path+\"/data/val/classificationA\"\n",
        "\n",
        "traindirB = dir_path+\"/data/train/classificationB\"\n",
        "valdirB = dir_path+\"/data/val/classificationB\"\n",
        "\n",
        "traindirC = dir_path+\"/data/train/classificationC\"\n",
        "valdirC = dir_path+\"/data/val/classificationC\"\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "val_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      ])\n",
        "\n",
        "\n",
        "def schematrain(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    mse = torch.nn.MSELoss()\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    pred_loss = 0.05*mse(pred_attn, h1m)\n",
        "    policy_loss = bce(policy, y)\n",
        "    total_loss = sum([pred_loss, policy_loss])\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return total_loss\n",
        "\n",
        "def schematrain_policy(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def controltrain(model, x, y, optimizer):\n",
        "    h1, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def fitschema(model, trainloader, valloader, name=\"\", n_epochs=20, policy_only=False):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): # iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float()\n",
        "          model.train()\n",
        "          if policy_only:\n",
        "            loss = schematrain_policy(model, x_batch, y_batch, optimizer)\n",
        "          else:\n",
        "            loss = schematrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float()\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          model.eval()\n",
        "\n",
        "          _, _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          total_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(total_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,total_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        # save best model\n",
        "        if total_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "def fitcontrol(model, trainloader, valloader, name=\"\", n_epochs=20):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): # iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float()\n",
        "          model.train()\n",
        "          loss = controltrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float()\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          model.eval()\n",
        "\n",
        "          _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          total_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(total_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,total_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        # save best model\n",
        "        if total_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "def evaluate(model, valloader, name=\"\", save_attn=False):\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  model.eval()\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  total_acc = 0\n",
        "  for i, data in enumerate(valloader):\n",
        "      accuracy = 0\n",
        "      x_batch, y_batch = data\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float()\n",
        "      outputs = model.forward(x_batch)\n",
        "      policy = outputs[-1]\n",
        "      policy = torch.round(sigmoid(policy))\n",
        "      accuracy = 1-(torch.sum(abs(policy - y_batch))/len(y_batch))\n",
        "      total_acc += accuracy.item()/len(valloader)\n",
        "      for pol in policy:\n",
        "        classifications.append(pol.item())\n",
        "      for yb in y_batch:\n",
        "        labels.append(yb.item())\n",
        "\n",
        "  file = open(dir_path+\"/accuracy_\"+name+\".txt\",\"w\")\n",
        "  file.write(str(total_acc))\n",
        "  file.close()\n",
        "\n",
        "def freeze_models(models):\n",
        "  for i, model in enumerate(models):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.policy.parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seedn = 0\n",
        "seeds = [43250, 58038, 70991, 85884, 88252, 98122, 59732, 59721, 34361,\n",
        "         24375, 17167, 25532, 24606, 27055, 77062, 27850, 93109, 37718,\n",
        "         70332, 75087]\n",
        "\n",
        "for trial in range(0,20):\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "  tname = \"trial\"+str(trial)\n",
        "  # FIT / EVALUATE MODELS: IMAGE CLASSIFICATION\n",
        "  # A\n",
        "  train_dataA = datasets.ImageFolder(traindirA,transform=train_transforms)\n",
        "  val_dataA = datasets.ImageFolder(valdirA,transform=val_transforms)\n",
        "\n",
        "  trainloaderA = torch.utils.data.DataLoader(train_dataA, shuffle = True, batch_size=8)\n",
        "  valloaderA = torch.utils.data.DataLoader(val_dataA, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelAschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelAcontrol = vision_transformer.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelAschema, trainloaderA, valloaderA, tname+\"A\")\n",
        "  evaluate(modelAschema, valloaderA, tname+\"Aschema\", save_attn=False)\n",
        "  fitcontrol(modelAcontrol, trainloaderA, valloaderA, tname+\"A\")\n",
        "  evaluate(modelAcontrol, valloaderA, tname+\"Acontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataA)\n",
        "  del(val_dataA)\n",
        "  del(trainloaderA)\n",
        "  del(valloaderA)\n",
        "\n",
        "\n",
        "  # B\n",
        "  train_dataB = datasets.ImageFolder(traindirB,transform=train_transforms)\n",
        "  val_dataB = datasets.ImageFolder(valdirB,transform=val_transforms)\n",
        "\n",
        "  trainloaderB = torch.utils.data.DataLoader(train_dataB, shuffle = True, batch_size=8)\n",
        "  valloaderB = torch.utils.data.DataLoader(val_dataB, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelBschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelBcontrol = vision_transformer.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelBschema, trainloaderB, valloaderB, tname+\"B\")\n",
        "  evaluate(modelBschema, valloaderB, tname+\"Bschema\", save_attn=False)\n",
        "  fitcontrol(modelBcontrol, trainloaderB, valloaderB, tname+\"B\")\n",
        "  evaluate(modelBcontrol, valloaderB, tname+\"Bcontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataB)\n",
        "  del(val_dataB)\n",
        "  del(trainloaderB)\n",
        "  del(valloaderB)\n",
        "\n",
        "  # C\n",
        "  train_dataC = datasets.ImageFolder(traindirC,transform=train_transforms)\n",
        "  val_dataC = datasets.ImageFolder(valdirC,transform=val_transforms)\n",
        "\n",
        "  trainloaderC = torch.utils.data.DataLoader(train_dataC, shuffle = True, batch_size=8)\n",
        "  valloaderC = torch.utils.data.DataLoader(val_dataC, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelCschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelCcontrol = vision_transformer.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelCschema, trainloaderC, valloaderC, tname+\"C\")\n",
        "  evaluate(modelCschema, valloaderC, tname+\"Cschema\", save_attn=False)\n",
        "  fitcontrol(modelCcontrol, trainloaderC, valloaderC, tname+\"C\")\n",
        "  evaluate(modelCcontrol, valloaderC, tname+\"Ccontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataC)\n",
        "  del(val_dataC)\n",
        "  del(trainloaderC)\n",
        "  del(valloaderC)\n",
        "\n",
        "  # FREEZE THE MODELS\n",
        "  freeze_models([modelAschema, modelAcontrol, modelBschema, modelBcontrol, modelCschema, modelCcontrol])\n",
        "\n",
        "  # TRANSFER LEARNING\n",
        "  train_dataB = datasets.ImageFolder(traindirB,transform=train_transforms)\n",
        "  val_dataB = datasets.ImageFolder(valdirB,transform=val_transforms)\n",
        "\n",
        "  trainloaderB = torch.utils.data.DataLoader(train_dataB, shuffle = True, batch_size=8)\n",
        "  valloaderB = torch.utils.data.DataLoader(val_dataB, shuffle = True, batch_size=8)\n",
        "\n",
        "  fitschema(modelAschema, trainloaderB, valloaderB, tname+\"AschemaB\", policy_only=True)\n",
        "  evaluate(modelAschema, valloaderB, tname+\"AschemaB\")\n",
        "\n",
        "  fitcontrol(modelAcontrol, trainloaderB, valloaderB, tname+\"AcontrolB\")\n",
        "  evaluate(modelAschema, valloaderB, tname+\"AcontrolB\")\n",
        "\n",
        "  del(train_dataB)\n",
        "  del(val_dataB)\n",
        "  del(trainloaderB)\n",
        "  del(valloaderB)\n",
        "\n",
        "  del(modelAschema)\n",
        "  del(modelAcontrol)\n",
        "\n",
        "  train_dataC = datasets.ImageFolder(traindirC,transform=train_transforms)\n",
        "  val_dataC = datasets.ImageFolder(valdirC,transform=val_transforms)\n",
        "\n",
        "  trainloaderC = torch.utils.data.DataLoader(train_dataC, shuffle = True, batch_size=8)\n",
        "  valloaderC = torch.utils.data.DataLoader(val_dataC, shuffle = True, batch_size=8)\n",
        "\n",
        "  fitschema(modelBschema, trainloaderC, valloaderC, tname+\"BschemaC\", policy_only=True)\n",
        "  evaluate(modelBschema, valloaderC, tname+\"BschemaC\")\n",
        "\n",
        "  fitcontrol(modelBcontrol, trainloaderC, valloaderC, tname+\"BcontrolC\")\n",
        "  evaluate(modelBcontrol, valloaderC, tname+\"BcontrolC\")\n",
        "\n",
        "  del(train_dataC)\n",
        "  del(val_dataC)\n",
        "  del(trainloaderC)\n",
        "  del(valloaderC)\n",
        "\n",
        "  del(modelBschema)\n",
        "  del(modelBcontrol)\n",
        "\n",
        "  train_dataA = datasets.ImageFolder(traindirA,transform=train_transforms)\n",
        "  val_dataA = datasets.ImageFolder(valdirA,transform=val_transforms)\n",
        "\n",
        "  trainloaderA = torch.utils.data.DataLoader(train_dataA, shuffle = True, batch_size=8)\n",
        "  valloaderA = torch.utils.data.DataLoader(val_dataA, shuffle = True, batch_size=8)\n",
        "\n",
        "  fitschema(modelCschema, trainloaderA, valloaderA, tname+\"CschemaA\", policy_only=True)\n",
        "  evaluate(modelCschema, valloaderA, tname+\"CschemaA\")\n",
        "\n",
        "  fitcontrol(modelCcontrol, trainloaderA, valloaderA, tname+\"CcontrolA\")\n",
        "  evaluate(modelCcontrol, valloaderA, tname+\"CcontrolA\")\n",
        "\n",
        "  del(train_dataA)\n",
        "  del(val_dataA)\n",
        "  del(trainloaderA)\n",
        "  del(valloaderA)\n",
        "\n",
        "  del(modelCschema)\n",
        "  del(modelCcontrol)\n",
        "\n",
        "  seedn += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "wvGJaXMs5v25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22e01986-aaa4-4fd4-af03-5d636656a9e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 0.679358959197998 / 239: 0.002842506105430954\n",
            "1: 0.8153344988822937 / 239: 0.006253947523348501\n",
            "2: 0.659233570098877 / 239: 0.009012246979829157\n",
            "3: 0.7260761260986328 / 239: 0.01205022240283599\n",
            "4: 0.6982764005661011 / 239: 0.014971880982610472\n",
            "5: 0.7147909998893738 / 239: 0.017962638304323333\n",
            "6: 0.6804316639900208 / 239: 0.02080963271432342\n",
            "7: 0.7297563552856445 / 239: 0.023863006585811475\n",
            "8: 0.6901165843009949 / 239: 0.026750523674936976\n",
            "9: 0.6755895614624023 / 239: 0.02957725824172527\n",
            "10: 0.6857888102531433 / 239: 0.03244666748964637\n",
            "11: 0.6495785117149353 / 239: 0.035164569212303\n",
            "12: 0.6728949546813965 / 239: 0.03798002927373144\n",
            "13: 0.664060652256012 / 239: 0.040758525726685466\n",
            "14: 0.6826902627944946 / 239: 0.04361497034088837\n",
            "15: 0.6372897624969482 / 239: 0.04628145470280029\n",
            "16: 0.6490436792373657 / 239: 0.04899711863266375\n",
            "17: 0.6892343163490295 / 239: 0.05188094422408228\n",
            "18: 0.5775551795959473 / 239: 0.05429749309268457\n",
            "19: 0.6542345285415649 / 239: 0.057034876057293626\n",
            "20: 0.5902770161628723 / 239: 0.05950465436759853\n",
            "21: 0.6247652769088745 / 239: 0.06211873502412102\n",
            "22: 0.6588501334190369 / 239: 0.06487543014302913\n",
            "23: 0.6319486498832703 / 239: 0.06751956675341939\n",
            "24: 0.6680779457092285 / 239: 0.07031487196559189\n",
            "25: 0.5619166493415833 / 239: 0.07266598765321357\n",
            "26: 0.6145012974739075 / 239: 0.07523712278908766\n",
            "27: 0.5798464417457581 / 239: 0.07766325852860965\n",
            "28: 0.5147435665130615 / 239: 0.07981699730063083\n",
            "29: 0.54311203956604 / 239: 0.08208943261262262\n",
            "30: 0.5951662063598633 / 239: 0.08457966778567644\n",
            "31: 0.4953984320163727 / 239: 0.08665246457235583\n",
            "32: 0.4858725070953369 / 239: 0.08868540393258736\n",
            "33: 0.7190152406692505 / 239: 0.09169383590191477\n",
            "34: 0.35677096247673035 / 239: 0.0931866014352902\n",
            "35: 0.4450450539588928 / 239: 0.09504871463177092\n",
            "36: 0.5945887565612793 / 239: 0.09753653369688088\n",
            "37: 0.3249139189720154 / 239: 0.09889600616119894\n",
            "38: 0.7828578948974609 / 239: 0.102171562206795\n",
            "39: 0.6956921219825745 / 239: 0.10508240790546687\n",
            "40: 0.8566045761108398 / 239: 0.10866652747078419\n",
            "41: 0.30918601155281067 / 239: 0.10996019279108883\n",
            "42: 0.6147475242614746 / 239: 0.1125323581645678\n",
            "43: 0.3014189898967743 / 239: 0.11379352548631162\n",
            "44: 0.40200620889663696 / 239: 0.11547555983315948\n",
            "45: 0.7595657706260681 / 239: 0.11865365929184596\n",
            "46: 0.44752073287963867 / 239: 0.12052613097753483\n",
            "47: 0.21017177402973175 / 239: 0.1214055107851906\n",
            "48: 1.0913416147232056 / 239: 0.12597179369198225\n",
            "49: 0.3677130341529846 / 239: 0.1275103419520366\n",
            "50: 0.5949659943580627 / 239: 0.1299997394179699\n",
            "51: 0.42238160967826843 / 239: 0.13176702648775346\n",
            "52: 0.31336620450019836 / 239: 0.1330781821551183\n",
            "53: 0.3288799822330475 / 239: 0.13445424902638628\n",
            "54: 0.46166136860847473 / 239: 0.13638588655194475\n",
            "55: 0.7584360241889954 / 239: 0.1395592590380912\n",
            "56: 0.8393131494522095 / 239: 0.14307102953789122\n",
            "57: 0.521645724773407 / 239: 0.14525364763317744\n",
            "58: 0.5029957294464111 / 239: 0.14735823227521264\n",
            "59: 0.8184394240379333 / 239: 0.15078266501177304\n",
            "60: 0.5929743647575378 / 239: 0.15326372929946147\n",
            "61: 0.4106188416481018 / 239: 0.15498179976660834\n",
            "62: 0.27945029735565186 / 239: 0.15615104787269893\n",
            "63: 0.8738153576850891 / 239: 0.15980717907640224\n",
            "64: 0.5147494673728943 / 239: 0.16196094253821353\n",
            "65: 0.44096192717552185 / 239: 0.16380597152221152\n",
            "66: 0.8726405501365662 / 239: 0.1674571872131595\n",
            "67: 0.6931079626083374 / 239: 0.17035722052951238\n",
            "68: 0.4126436114311218 / 239: 0.17208376283675558\n",
            "69: 0.43817901611328125 / 239: 0.17391714784141366\n",
            "70: 0.5032540559768677 / 239: 0.176022813347593\n",
            "71: 0.4452516734600067 / 239: 0.17788579106081479\n",
            "72: 0.3496895730495453 / 239: 0.17934892734972502\n",
            "73: 0.4680279791355133 / 239: 0.18130720341305354\n",
            "74: 0.3778479993343353 / 239: 0.18288815738516373\n",
            "75: 0.3423866033554077 / 239: 0.1843207373155211\n",
            "76: 0.34877774119377136 / 239: 0.18578005840838205\n",
            "77: 0.506862223148346 / 239: 0.1879008208483333\n",
            "78: 0.40972185134887695 / 239: 0.18961513821799386\n",
            "79: 0.3154670298099518 / 239: 0.19093508394941625\n",
            "80: 0.46652424335479736 / 239: 0.19288706823123547\n",
            "81: 0.5459092855453491 / 239: 0.19517120750129968\n",
            "82: 0.5980690121650696 / 239: 0.19767358830533763\n",
            "83: 0.32562482357025146 / 239: 0.19903603526588262\n",
            "84: 0.49755409359931946 / 239: 0.20111785155709316\n",
            "85: 0.6304672956466675 / 239: 0.20375579003260225\n",
            "86: 0.4876812994480133 / 239: 0.20579629756167342\n",
            "87: 0.3773930072784424 / 239: 0.20737534780133218\n",
            "88: 0.5126032829284668 / 239: 0.20952013141191153\n",
            "89: 1.1038024425506592 / 239: 0.21413855167362977\n",
            "90: 0.1938503235578537 / 239: 0.2149496408935371\n",
            "91: 0.22616522014141083 / 239: 0.21589593888576059\n",
            "92: 0.4235541522502899 / 239: 0.21766813199141033\n",
            "93: 0.11273882538080215 / 239: 0.2181398425578572\n",
            "94: 0.3827870190143585 / 239: 0.21974146188427712\n",
            "95: 0.47628387808799744 / 239: 0.22173428145786708\n",
            "96: 0.17555473744869232 / 239: 0.22246882010828\n",
            "97: 0.5917862057685852 / 239: 0.22494491301944564\n",
            "98: 0.3518860340118408 / 239: 0.22641723952158724\n",
            "99: 0.5036249160766602 / 239: 0.2285244567436653\n",
            "100: 0.34189972281455994 / 239: 0.22995499951694798\n",
            "101: 0.344882607460022 / 239: 0.2313980229791238\n",
            "102: 0.3158048987388611 / 239: 0.232719382388073\n",
            "103: 0.2466694712638855 / 239: 0.2337514722259972\n",
            "104: 0.3446657061576843 / 239: 0.23519358815134317\n",
            "105: 1.029175877571106 / 239: 0.23949976337130596\n",
            "106: 0.2538440227508545 / 239: 0.2405618722531087\n",
            "107: 0.6435064077377319 / 239: 0.2432543676829737\n",
            "108: 0.681525707244873 / 239: 0.24610593967981417\n",
            "109: 0.16666895151138306 / 239: 0.2468032993095689\n",
            "110: 0.1793101280927658 / 239: 0.24755355089154701\n",
            "111: 0.40678665041923523 / 239: 0.24925558708576975\n",
            "112: 0.2336874008178711 / 239: 0.25023335863730894\n",
            "113: 0.3149186670780182 / 239: 0.2515510099639952\n",
            "114: 0.31121477484703064 / 239: 0.25285316383364803\n",
            "115: 0.17828799784183502 / 239: 0.25359913872001555\n",
            "116: 0.4085942208766937 / 239: 0.25530873797054565\n",
            "117: 0.2621912360191345 / 239: 0.25640577243087675\n",
            "118: 0.16079863905906677 / 239: 0.25707857008384355\n",
            "119: 0.5805698037147522 / 239: 0.25950773244248265\n",
            "120: 0.3809821605682373 / 239: 0.26110180005992295\n",
            "121: 0.2033931314945221 / 239: 0.26195281734651094\n",
            "122: 0.17711985111236572 / 239: 0.2626939045896589\n",
            "123: 0.3065260648727417 / 239: 0.26397644042594653\n",
            "124: 0.30233898758888245 / 239: 0.26524145711041885\n",
            "125: 0.11712875217199326 / 239: 0.2657315355713895\n",
            "126: 0.3049805760383606 / 239: 0.26700760492719855\n",
            "127: 0.21155259013175964 / 239: 0.26789276220808456\n",
            "128: 0.23409712314605713 / 239: 0.2688722480789886\n",
            "129: 0.426093727350235 / 239: 0.2706550670218766\n",
            "130: 0.1391177773475647 / 239: 0.2712371497722848\n",
            "131: 0.04112646356225014 / 239: 0.2714092270256833\n",
            "132: 0.21440067887306213 / 239: 0.2723063009958635\n",
            "133: 0.17316164076328278 / 239: 0.27303082668943374\n",
            "134: 0.6146910190582275 / 239: 0.2756027556394682\n",
            "135: 0.23053425550460815 / 239: 0.27656733411438283\n",
            "136: 0.502197265625 / 239: 0.27866857790360877\n",
            "137: 0.5942158102989197 / 239: 0.28115483652410633\n",
            "138: 0.06685039401054382 / 239: 0.2814345452856567\n",
            "139: 0.12799237668514252 / 239: 0.2819700782424983\n",
            "140: 0.19945448637008667 / 239: 0.28280461584237315\n",
            "141: 0.792664110660553 / 239: 0.2861212020794466\n",
            "142: 0.39819467067718506 / 239: 0.28778728856763564\n",
            "143: 0.2962440550327301 / 239: 0.28902680344224957\n",
            "144: 0.40401577949523926 / 239: 0.29071724603427984\n",
            "145: 0.2281772643327713 / 239: 0.29167196262144623\n",
            "146: 0.42677947878837585 / 239: 0.2934576508172135\n",
            "147: 0.4023301899433136 / 239: 0.2951410407332943\n",
            "148: 0.2834886312484741 / 239: 0.2963271856339155\n",
            "149: 0.6810524463653564 / 239: 0.299176777459712\n",
            "150: 0.1469959318637848 / 239: 0.29979182319972786\n",
            "151: 0.33316877484321594 / 239: 0.30118583480995054\n",
            "152: 0.12421978265047073 / 239: 0.30170558285451315\n",
            "153: 0.2489721179008484 / 239: 0.3027473071971945\n",
            "154: 0.16074462234973907 / 239: 0.3034198788388252\n",
            "155: 0.5913229584693909 / 239: 0.30589403347677246\n",
            "156: 0.10730717331171036 / 239: 0.3063430174655244\n",
            "157: 0.11427336931228638 / 239: 0.3068211487178771\n",
            "158: 0.14734673500061035 / 239: 0.3074376622534445\n",
            "159: 0.2854594588279724 / 239: 0.3086320532945657\n",
            "160: 0.6801908612251282 / 239: 0.31147804016161645\n",
            "161: 0.42347297072410583 / 239: 0.3132498935956085\n",
            "162: 0.20739436149597168 / 239: 0.3141176524303197\n",
            "163: 0.38084277510643005 / 239: 0.31571113684499097\n",
            "164: 0.49140411615371704 / 239: 0.31776722101299815\n",
            "165: 0.2812283933162689 / 239: 0.31894390885114154\n",
            "166: 0.6485105752944946 / 239: 0.32165734222057457\n",
            "167: 0.6609580516815186 / 239: 0.3244228570811667\n",
            "168: 0.6031785011291504 / 239: 0.32694661650011714\n",
            "169: 0.18536031246185303 / 239: 0.3277221826610454\n",
            "170: 0.0999460369348526 / 239: 0.32814036691600296\n",
            "171: 0.3057194650173187 / 239: 0.329419527857498\n",
            "172: 0.22763100266456604 / 239: 0.3303719588309899\n",
            "173: 0.8292176723480225 / 239: 0.33384148884081427\n",
            "174: 0.5291383862495422 / 239: 0.3360554569841178\n",
            "175: 0.496095210313797 / 239: 0.338131169161163\n",
            "176: 0.1254887729883194 / 239: 0.3386562267887292\n",
            "177: 0.3476077616214752 / 239: 0.3401106525695722\n",
            "178: 0.21272388100624084 / 239: 0.3410007106490962\n",
            "179: 0.1782127320766449 / 239: 0.3417463706159441\n",
            "180: 0.5658317804336548 / 239: 0.34411386760520624\n",
            "181: 0.4874315857887268 / 239: 0.3461533303072511\n",
            "182: 0.15227796137332916 / 239: 0.34679047658914786\n",
            "183: 0.1468694508075714 / 239: 0.34740499311972345\n",
            "184: 0.17486175894737244 / 239: 0.34813663227849906\n",
            "185: 0.32013463973999023 / 239: 0.3494761077585827\n",
            "186: 0.11431651562452316 / 239: 0.3499544195394384\n",
            "187: 0.40148377418518066 / 239: 0.35163426796699143\n",
            "188: 0.6268119812011719 / 239: 0.35425691223979966\n",
            "189: 0.3721027076244354 / 239: 0.3558138273344626\n",
            "190: 0.32923227548599243 / 239: 0.3571913682360776\n",
            "191: 0.11363612860441208 / 239: 0.3576668332093178\n",
            "192: 0.2561708092689514 / 239: 0.35873867759956446\n",
            "193: 0.39679229259490967 / 239: 0.3603988963970327\n",
            "194: 0.24575184285640717 / 239: 0.36142714678555327\n",
            "195: 0.4024113118648529 / 239: 0.3631108761238999\n",
            "196: 0.10731164366006851 / 239: 0.3635598788170383\n",
            "197: 0.24989569187164307 / 239: 0.3646054674859573\n",
            "198: 0.4753592908382416 / 239: 0.36659441849364865\n",
            "199: 0.09658670425415039 / 239: 0.3669985469633313\n",
            "200: 0.7084720730781555 / 239: 0.3699628652607294\n",
            "201: 0.37191706895828247 / 239: 0.3715190036245716\n",
            "202: 0.23424208164215088 / 239: 0.37249909601637976\n",
            "203: 0.5117822885513306 / 239: 0.3746404445040422\n",
            "204: 0.35368382930755615 / 239: 0.37612029316223283\n",
            "205: 0.3435291647911072 / 239: 0.37755765368437133\n",
            "206: 0.24782520532608032 / 239: 0.3785945792296687\n",
            "207: 0.6279496550559998 / 239: 0.38122198364412896\n",
            "208: 0.6064971685409546 / 239: 0.38375962870078567\n",
            "209: 0.05813497677445412 / 239: 0.3840028712814319\n",
            "210: 0.7460752725601196 / 239: 0.3871245251415161\n",
            "211: 0.8672518134117126 / 239: 0.3907531938168789\n",
            "212: 0.8718006610870361 / 239: 0.39440089532770334\n",
            "213: 0.2684129774570465 / 239: 0.39552396217898805\n",
            "214: 0.4391118586063385 / 239: 0.39736125029031166\n",
            "215: 0.43855103850364685 / 239: 0.39919619187400895\n",
            "216: 0.31223124265670776 / 239: 0.4005025987470496\n",
            "217: 0.8189112544059753 / 239: 0.40392900566925033\n",
            "218: 0.26848211884498596 / 239: 0.4050523618150452\n",
            "219: 0.5935680270195007 / 239: 0.40753591004525236\n",
            "220: 0.14172670245170593 / 239: 0.4081289088002804\n",
            "221: 0.4149056673049927 / 239: 0.40986491577645195\n",
            "222: 0.22938890755176544 / 239: 0.4108247020005179\n",
            "223: 0.13531813025474548 / 239: 0.41139088664593526\n",
            "224: 0.3662528991699219 / 239: 0.41292332555459604\n",
            "225: 0.48714181780815125 / 239: 0.41496157583831217\n",
            "226: 0.31245288252830505 / 239: 0.4162689100748323\n",
            "227: 0.2258257120847702 / 239: 0.41721378753125393\n",
            "228: 0.2933829724788666 / 239: 0.4184413313491571\n",
            "229: 0.8028758764266968 / 239: 0.4218006446396454\n",
            "230: 0.8867174983024597 / 239: 0.4255107596953042\n",
            "231: 0.2787608504295349 / 239: 0.4266771230862228\n",
            "232: 0.3734592795372009 / 239: 0.42823971421399354\n",
            "233: 0.47093307971954346 / 239: 0.43021014550989123\n",
            "234: 0.5564642548561096 / 239: 0.4325384478314649\n",
            "235: 0.5315657258033752 / 239: 0.4347625722072112\n",
            "236: 0.1428331434726715 / 239: 0.43536020042257806\n",
            "237: 0.22905240952968597 / 239: 0.436318578705129\n",
            "238: 0.44832080602645874 / 239: 0.4381943979772063\n",
            "\n",
            "Epoch : 1, train loss : 0.4381943979772063\n",
            "Epoch : 1, val loss : 0.31440070725005603\n",
            "\n",
            "Epoch : 2, train loss : 0.2627824199226172\n",
            "Epoch : 2, val loss : 0.2257671982616834\n",
            "\n",
            "Epoch : 3, train loss : 0.20634215690632\n",
            "Epoch : 3, val loss : 0.26970603701699036\n",
            "\n",
            "Epoch : 4, train loss : 0.18193008441859893\n",
            "Epoch : 4, val loss : 0.2474006537510001\n",
            "\n",
            "Epoch : 5, train loss : 0.1537442439777394\n",
            "Epoch : 5, val loss : 0.21158040699589506\n",
            "\n",
            "Epoch : 6, train loss : 0.12739649444826284\n",
            "Epoch : 6, val loss : 0.2241627315575581\n",
            "\n",
            "Epoch : 7, train loss : 0.12093671128566857\n",
            "Epoch : 7, val loss : 0.31196444824783376\n",
            "\n",
            "Epoch : 8, train loss : 0.09390910970783599\n",
            "Epoch : 8, val loss : 0.243085557817603\n",
            "\n",
            "Epoch : 9, train loss : 0.06835280230637078\n",
            "Epoch : 9, val loss : 0.2949759669155237\n",
            "\n",
            "Epoch : 10, train loss : 0.0766431335579004\n",
            "Epoch : 10, val loss : 0.3470140414397263\n",
            "\n",
            "Epoch : 11, train loss : 0.05712066877294316\n",
            "Epoch : 11, val loss : 0.426679539004592\n",
            "\n",
            "Epoch : 12, train loss : 0.06914823683890585\n",
            "Epoch : 12, val loss : 0.40742088436040236\n",
            "\n",
            "Epoch : 13, train loss : 0.06186833288096803\n",
            "Epoch : 13, val loss : 0.3240179095780109\n",
            "\n",
            "Epoch : 14, train loss : 0.028753412185065347\n",
            "Epoch : 14, val loss : 0.38694123447925455\n",
            "\n",
            "Epoch : 15, train loss : 0.058072924407912126\n",
            "Epoch : 15, val loss : 0.48399061935513454\n",
            "\n",
            "Epoch : 16, train loss : 0.0666514699806159\n",
            "Epoch : 16, val loss : 0.34615710212354955\n",
            "\n",
            "Epoch : 17, train loss : 0.049084204187710256\n",
            "Epoch : 17, val loss : 0.40659886420326063\n",
            "\n",
            "Epoch : 18, train loss : 0.030941832323546938\n",
            "Epoch : 18, val loss : 0.42449516473761445\n",
            "\n",
            "Epoch : 19, train loss : 0.044604544151792574\n",
            "Epoch : 19, val loss : 0.6335326308051259\n",
            "\n",
            "Epoch : 20, train loss : 0.06615283514351565\n",
            "Epoch : 20, val loss : 0.3327218107762113\n",
            "0: 0.686403214931488 / 239: 0.002871979978792837\n",
            "1: 0.6395205855369568 / 239: 0.005547798328319853\n",
            "2: 0.5750618577003479 / 239: 0.007953914887735534\n",
            "3: 0.6408358812332153 / 239: 0.010635236566535599\n",
            "4: 0.7452192902565002 / 239: 0.013753308910705055\n",
            "5: 0.5035616159439087 / 239: 0.0158602612786712\n",
            "6: 0.4244934916496277 / 239: 0.01763638467469475\n",
            "7: 0.18525588512420654 / 239: 0.018411513901155866\n",
            "8: 1.0890840291976929 / 239: 0.02296835084340563\n",
            "9: 0.48893851041793823 / 239: 0.025014118669422106\n",
            "10: 0.25737255811691284 / 239: 0.026090991297526345\n",
            "11: 0.3746315836906433 / 239: 0.027658487463595983\n",
            "12: 0.8320212960243225 / 239: 0.031139748116417416\n",
            "13: 0.8923588991165161 / 239: 0.03487346735958276\n",
            "14: 0.20414479076862335 / 239: 0.03572762966405398\n",
            "15: 0.5447616577148438 / 239: 0.03800696714403241\n",
            "16: 0.19802974164485931 / 239: 0.038835543468906296\n",
            "17: 0.9695245027542114 / 239: 0.04289213134653898\n",
            "18: 0.31661713123321533 / 239: 0.04421688921780766\n",
            "19: 0.5617758631706238 / 239: 0.04656741584195253\n",
            "20: 0.3641943633556366 / 239: 0.04809124163005143\n",
            "21: 0.30757972598075867 / 239: 0.049378186090221966\n",
            "22: 0.4683682918548584 / 239: 0.051337886056141875\n",
            "23: 0.6508548855781555 / 239: 0.054061128255213656\n",
            "24: 0.3689560294151306 / 239: 0.055604877332264414\n",
            "25: 0.2805308401584625 / 239: 0.056778646537948355\n",
            "26: 0.39263808727264404 / 239: 0.058421483723189546\n",
            "27: 0.4772363007068634 / 239: 0.060418288328657595\n",
            "28: 0.3075348436832428 / 239: 0.06170504499678832\n",
            "29: 0.2876262366771698 / 239: 0.06290850205401496\n",
            "30: 0.4109821915626526 / 239: 0.06462809281369133\n",
            "31: 0.6162376403808594 / 239: 0.0672064929826489\n",
            "32: 0.18919946253299713 / 239: 0.06799812253299617\n",
            "33: 0.28746122121810913 / 239: 0.06920088914897152\n",
            "34: 0.5317293405532837 / 239: 0.0714256981052614\n",
            "35: 0.18290989100933075 / 239: 0.07219101145676489\n",
            "36: 0.34010207653045654 / 239: 0.0736140326974781\n",
            "37: 0.4475237727165222 / 239: 0.07548651710214974\n",
            "38: 0.4163585305213928 / 239: 0.07722860300391289\n",
            "39: 0.39041221141815186 / 239: 0.07886212690105997\n",
            "40: 0.30914008617401123 / 239: 0.08015560006496797\n",
            "41: 0.12696072459220886 / 239: 0.08068681648585588\n",
            "42: 0.49544668197631836 / 239: 0.08275981515521286\n",
            "43: 0.2964686453342438 / 239: 0.08400026973820132\n",
            "44: 0.2656630873680115 / 239: 0.08511183077321392\n",
            "45: 0.22616419196128845 / 239: 0.08605812446342852\n",
            "46: 0.5242924094200134 / 239: 0.088251816553052\n",
            "47: 1.9691895246505737 / 239: 0.09649110326707114\n",
            "48: 0.21879439055919647 / 239: 0.09740656096815564\n",
            "49: 0.045539237558841705 / 239: 0.09759710171107967\n",
            "50: 0.39706727862358093 / 239: 0.09925847107770554\n",
            "51: 0.7749313712120056 / 239: 0.10250086175223276\n",
            "52: 0.2714988589286804 / 239: 0.1036368402414741\n",
            "53: 0.5842795372009277 / 239: 0.10608152449754493\n",
            "54: 0.35522016882896423 / 239: 0.10756780135456989\n",
            "55: 0.26125457882881165 / 239: 0.10866091674715907\n",
            "56: 0.8312849998474121 / 239: 0.11213909666283861\n",
            "57: 0.3133721351623535 / 239: 0.11345027714468946\n",
            "58: 0.2599411606788635 / 239: 0.11453789706384788\n",
            "59: 0.3936662971973419 / 239: 0.11618503638266521\n",
            "60: 0.8585106134414673 / 239: 0.11977713099957511\n",
            "61: 0.4627496004104614 / 239: 0.1217133217962716\n",
            "62: 0.5348633527755737 / 239: 0.12395124377441208\n",
            "63: 0.7477722764015198 / 239: 0.1270799980689791\n",
            "64: 0.37856152653694153 / 239: 0.12866393751055627\n",
            "65: 0.6396100521087646 / 239: 0.1313401301972038\n",
            "66: 0.3545929491519928 / 239: 0.1328237827041159\n",
            "67: 0.605871319770813 / 239: 0.13535880914667162\n",
            "68: 0.6285541653633118 / 239: 0.13798874289296162\n",
            "69: 0.8378652334213257 / 239: 0.141494455166691\n",
            "70: 0.51064133644104 / 239: 0.14363102979615144\n",
            "71: 0.480319082736969 / 239: 0.1456407330712015\n",
            "72: 0.3922773003578186 / 239: 0.14728206068776142\n",
            "73: 0.6844301223754883 / 239: 0.15014578504916515\n",
            "74: 0.5381243228912354 / 239: 0.152397351253731\n",
            "75: 0.5008940696716309 / 239: 0.15449314234022318\n",
            "76: 0.640305757522583 / 239: 0.15717224592818377\n",
            "77: 0.6056866645812988 / 239: 0.15970649975488377\n",
            "78: 0.30209916830062866 / 239: 0.16097051301137175\n",
            "79: 0.2765492796897888 / 239: 0.1621276229682328\n",
            "80: 0.3539162278175354 / 239: 0.16360844400512625\n",
            "81: 0.2813671827316284 / 239: 0.16478571255212052\n",
            "82: 0.2563416063785553 / 239: 0.16585827157462493\n",
            "83: 0.8914922475814819 / 239: 0.16958836466073993\n",
            "84: 0.5615066289901733 / 239: 0.17193776478203773\n",
            "85: 0.40985774993896484 / 239: 0.17365265076504596\n",
            "86: 0.6120182275772095 / 239: 0.17621339648712633\n",
            "87: 0.17205648124217987 / 239: 0.1769332980822819\n",
            "88: 0.19597207009792328 / 239: 0.1777532649027753\n",
            "89: 0.13273383677005768 / 239: 0.17830863660474205\n",
            "90: 0.34149792790412903 / 239: 0.17973749822777188\n",
            "91: 0.6798171997070312 / 239: 0.18258192165750842\n",
            "92: 0.4283596873283386 / 239: 0.1843742216044889\n",
            "93: 0.7903929948806763 / 239: 0.1876813052650775\n",
            "94: 0.09169203042984009 / 239: 0.18806495392796388\n",
            "95: 0.8112823963165283 / 239: 0.19145944094184056\n",
            "96: 0.47221308946609497 / 239: 0.19343522792705434\n",
            "97: 1.257401466369629 / 239: 0.1986963219286009\n",
            "98: 0.2195923626422882 / 239: 0.19961511842501214\n",
            "99: 0.420356810092926 / 239: 0.20137393353000346\n",
            "100: 0.5180944800376892 / 239: 0.2035416928607051\n",
            "101: 0.45605066418647766 / 239: 0.2054498546355439\n",
            "102: 0.5275013446807861 / 239: 0.20765697323253465\n",
            "103: 0.34035640954971313 / 239: 0.20908105862814014\n",
            "104: 0.7170946002006531 / 239: 0.21208145444487927\n",
            "105: 0.8229221701622009 / 239: 0.21552464344137384\n",
            "106: 0.48296913504600525 / 239: 0.21754543480139896\n",
            "107: 0.5115244388580322 / 239: 0.2196857044200518\n",
            "108: 0.4820546805858612 / 239: 0.22170266961078763\n",
            "109: 0.3599405884742737 / 239: 0.2232086971776256\n",
            "110: 0.6046119928359985 / 239: 0.22573845446982643\n",
            "111: 0.5782086849212646 / 239: 0.22815773767033382\n",
            "112: 0.5525982975959778 / 239: 0.23046986443851783\n",
            "113: 0.7452694177627563 / 239: 0.23358814652120719\n",
            "114: 0.33905041217803955 / 239: 0.23500676749266342\n",
            "115: 0.4713420271873474 / 239: 0.23697890986583223\n",
            "116: 0.5779992341995239 / 239: 0.23939731670348713\n",
            "117: 0.49118542671203613 / 239: 0.24145248585290988\n",
            "118: 0.77792888879776 / 239: 0.24470741844202185\n",
            "119: 0.525338888168335 / 239: 0.24690548910381405\n",
            "120: 0.5185048580169678 / 239: 0.2490749654971905\n",
            "121: 0.2883654534816742 / 239: 0.25028151551175815\n",
            "122: 0.4217866361141205 / 239: 0.252046313152403\n",
            "123: 0.6844922304153442 / 239: 0.25491029738008225\n",
            "124: 0.4711596369743347 / 239: 0.2568816766142845\n",
            "125: 0.4422903060913086 / 239: 0.2587322636690599\n",
            "126: 0.29687830805778503 / 239: 0.25997443232202133\n",
            "127: 0.18517379462718964 / 239: 0.2607492180735995\n",
            "128: 0.7293106317520142 / 239: 0.263800726993064\n",
            "129: 0.5175471305847168 / 239: 0.26596619615869044\n",
            "130: 0.4069603681564331 / 239: 0.2676689592053701\n",
            "131: 0.1973831206560135 / 239: 0.268494830003094\n",
            "132: 0.40963953733444214 / 239: 0.2702088029626523\n",
            "133: 0.2840959429740906 / 239: 0.2713974889165188\n",
            "134: 0.23731371760368347 / 239: 0.2723904333416388\n",
            "135: 0.16443438827991486 / 239: 0.2730784433344418\n",
            "136: 0.3109819293022156 / 239: 0.27437962295495316\n",
            "137: 0.4883808493614197 / 239: 0.27642305747110973\n",
            "138: 0.5695849657058716 / 239: 0.2788062581644397\n",
            "139: 0.7931495904922485 / 239: 0.2821248756978801\n",
            "140: 0.4260229468345642 / 239: 0.28390739848798285\n",
            "141: 0.6671714782714844 / 239: 0.28669891094936983\n",
            "142: 0.11010254174470901 / 239: 0.2871595910403519\n",
            "143: 0.3377934992313385 / 239: 0.2885729529618219\n",
            "144: 0.4424644410610199 / 239: 0.29042426861479687\n",
            "145: 0.11809355765581131 / 239: 0.2909183839187961\n",
            "146: 0.4545162618160248 / 239: 0.2928201256000347\n",
            "147: 0.2911471128463745 / 239: 0.29403831435671407\n",
            "148: 0.11328573524951935 / 239: 0.2945123132489715\n",
            "149: 0.31892821192741394 / 239: 0.29584674091393975\n",
            "150: 0.6969025135040283 / 239: 0.29876265101228294\n",
            "151: 0.3325123190879822 / 239: 0.30015391594570545\n",
            "152: 0.48438653349876404 / 239: 0.3021806378431898\n",
            "153: 0.4709892272949219 / 239: 0.3041513040661811\n",
            "154: 0.707506537437439 / 239: 0.30711158246550097\n",
            "155: 0.2682688534259796 / 239: 0.308234046287367\n",
            "156: 0.25908076763153076 / 239: 0.3093180662356161\n",
            "157: 0.21965128183364868 / 239: 0.3102371092558406\n",
            "158: 0.3506513833999634 / 239: 0.31170426985584043\n",
            "159: 0.4045793414115906 / 239: 0.31339707044752074\n",
            "160: 0.13822245597839355 / 239: 0.3139754070834136\n",
            "161: 0.37755921483039856 / 239: 0.31555515275216\n",
            "162: 0.2817332446575165 / 239: 0.3167339529390115\n",
            "163: 0.6791793704032898 / 239: 0.3195757076268914\n",
            "164: 0.22507789731025696 / 239: 0.32051745615120214\n",
            "165: 0.21383975446224213 / 239: 0.3214121831573203\n",
            "166: 0.35685598850250244 / 239: 0.32290530444812576\n",
            "167: 0.4542688727378845 / 239: 0.324806011028619\n",
            "168: 0.11506784707307816 / 239: 0.325287466455703\n",
            "169: 0.07732945680618286 / 239: 0.3256110206682812\n",
            "170: 0.6103835105895996 / 239: 0.3281649265703297\n",
            "171: 0.05919007956981659 / 239: 0.3284125838070235\n",
            "172: 0.15446452796459198 / 239: 0.32905887890310964\n",
            "173: 0.0739145502448082 / 239: 0.3293681448037155\n",
            "174: 0.058503102511167526 / 239: 0.32961292765941075\n",
            "175: 0.7412680387496948 / 239: 0.33271446757049733\n",
            "176: 0.07782772183418274 / 239: 0.33304010657398764\n",
            "177: 0.3449937403202057 / 239: 0.33448359502721026\n",
            "178: 0.3313990831375122 / 239: 0.33587020206962664\n",
            "179: 1.301978588104248 / 239: 0.34131781122487453\n",
            "180: 0.07224182784557343 / 239: 0.3416200782869899\n",
            "181: 0.42079105973243713 / 239: 0.34338071033607964\n",
            "182: 0.4654175341129303 / 239: 0.3453280640352969\n",
            "183: 0.16420310735702515 / 239: 0.3460151063254937\n",
            "184: 0.20200064778327942 / 239: 0.3468602973204028\n",
            "185: 0.2528514862060547 / 239: 0.34791825332963316\n",
            "186: 0.32561030983924866 / 239: 0.3492806395632702\n",
            "187: 0.5712754726409912 / 239: 0.3516709135073748\n",
            "188: 0.6360552310943604 / 239: 0.3543322324659286\n",
            "189: 0.6115372180938721 / 239: 0.3568909655960285\n",
            "190: 0.5635007619857788 / 239: 0.35924870937002756\n",
            "191: 0.24792851507663727 / 239: 0.3602860671736955\n",
            "192: 0.6708489060401917 / 239: 0.36309296636214816\n",
            "193: 0.5004703402519226 / 239: 0.36518698452219805\n",
            "194: 0.25154435634613037 / 239: 0.36623947136883456\n",
            "195: 0.3525070548057556 / 239: 0.36771439628433983\n",
            "196: 0.31967437267303467 / 239: 0.36905194596079605\n",
            "197: 0.14259500801563263 / 239: 0.3696485777934974\n",
            "198: 0.2661106288433075 / 239: 0.37076201138698406\n",
            "199: 0.35452258586883545 / 239: 0.37224536948685366\n",
            "200: 0.35451486706733704 / 239: 0.3737286952904827\n",
            "201: 0.17201799154281616 / 239: 0.37444843584087106\n",
            "202: 0.1081293374300003 / 239: 0.37490085984685434\n",
            "203: 0.14943233132362366 / 239: 0.3755260997268695\n",
            "204: 0.06790702790021896 / 239: 0.3758102295507198\n",
            "205: 0.18650363385677338 / 239: 0.3765905794831749\n",
            "206: 0.09227636456489563 / 239: 0.37697667305876026\n",
            "207: 0.04152446985244751 / 239: 0.37715041561044416\n",
            "208: 0.07104610651731491 / 239: 0.37744767965444964\n",
            "209: 0.44042420387268066 / 239: 0.3792904587501512\n",
            "210: 0.7004215717315674 / 239: 0.3822210929414967\n",
            "211: 0.7857775688171387 / 239: 0.3855088651959617\n",
            "212: 0.03656957298517227 / 239: 0.3856618759615901\n",
            "213: 0.26592740416526794 / 239: 0.38677454292462465\n",
            "214: 0.7627158761024475 / 239: 0.38996582274095287\n",
            "215: 0.9102311730384827 / 239: 0.39377432137291307\n",
            "216: 0.25006750226020813 / 239: 0.39482062891375075\n",
            "217: 0.35735952854156494 / 239: 0.39631585706664435\n",
            "218: 0.21350395679473877 / 239: 0.3972091790616014\n",
            "219: 0.26107800006866455 / 239: 0.3983015556309264\n",
            "220: 0.2498759627342224 / 239: 0.39934706175115325\n",
            "221: 0.52487713098526 / 239: 0.40154320037452257\n",
            "222: 0.9004267454147339 / 239: 0.40531067629675993\n",
            "223: 0.17549854516983032 / 239: 0.4060449798330354\n",
            "224: 0.5174858570098877 / 239: 0.40821019262387176\n",
            "225: 0.32544270157814026 / 239: 0.40957187756771335\n",
            "226: 0.42674246430397034 / 239: 0.4113574108911609\n",
            "227: 0.2707756757736206 / 239: 0.41249036350946056\n",
            "228: 0.8683562278747559 / 239: 0.41612365316584027\n",
            "229: 0.24219082295894623 / 239: 0.41713700388951785\n",
            "230: 0.36175286769866943 / 239: 0.4186506142146169\n",
            "231: 0.35043781995773315 / 239: 0.42011688124372876\n",
            "232: 0.6715953350067139 / 239: 0.4229269035659326\n",
            "233: 0.5954305529594421 / 239: 0.4254182447917043\n",
            "234: 0.2418287992477417 / 239: 0.4264300807718204\n",
            "235: 0.2584061920642853 / 239: 0.4275112782281563\n",
            "236: 0.26176461577415466 / 239: 0.4286065276665419\n",
            "237: 0.3373183608055115 / 239: 0.4300179015611256\n",
            "238: 0.24258208274841309 / 239: 0.431032889355052\n",
            "\n",
            "Epoch : 1, train loss : 0.431032889355052\n",
            "Epoch : 1, val loss : 0.2865352133369964\n",
            "\n",
            "Epoch : 2, train loss : 0.3044689495474974\n",
            "Epoch : 2, val loss : 0.28223758887337597\n",
            "\n",
            "Epoch : 3, train loss : 0.26962152395444194\n",
            "Epoch : 3, val loss : 0.23547789997056767\n",
            "\n",
            "Epoch : 4, train loss : 0.2480981112594198\n",
            "Epoch : 4, val loss : 0.20481825968169656\n",
            "\n",
            "Epoch : 5, train loss : 0.24706576050699502\n",
            "Epoch : 5, val loss : 0.3338678547221681\n",
            "\n",
            "Epoch : 6, train loss : 0.20340967231730162\n",
            "Epoch : 6, val loss : 0.5153637700714173\n",
            "\n",
            "Epoch : 7, train loss : 0.2196375151558401\n",
            "Epoch : 7, val loss : 0.27750833894369065\n",
            "\n",
            "Epoch : 8, train loss : 0.18993537232794153\n",
            "Epoch : 8, val loss : 0.25178764160955314\n",
            "\n",
            "Epoch : 9, train loss : 0.17340460738213925\n",
            "Epoch : 9, val loss : 0.21981781966093442\n",
            "\n",
            "Epoch : 10, train loss : 0.15724053211455635\n",
            "Epoch : 10, val loss : 0.18546197089650063\n",
            "\n",
            "Epoch : 11, train loss : 0.11495077072995395\n",
            "Epoch : 11, val loss : 0.24911352870454406\n",
            "\n",
            "Epoch : 12, train loss : 0.13519159986717413\n",
            "Epoch : 12, val loss : 0.2145416715951717\n",
            "\n",
            "Epoch : 13, train loss : 0.11988412716258397\n",
            "Epoch : 13, val loss : 0.21796017887232746\n",
            "\n",
            "Epoch : 14, train loss : 0.09648396825918608\n",
            "Epoch : 14, val loss : 0.3383825623674531\n",
            "\n",
            "Epoch : 15, train loss : 0.1217257069017315\n",
            "Epoch : 15, val loss : 0.22291036193390656\n",
            "\n",
            "Epoch : 16, train loss : 0.09935283959598698\n",
            "Epoch : 16, val loss : 0.32548707637065305\n",
            "\n",
            "Epoch : 17, train loss : 0.07606367859063931\n",
            "Epoch : 17, val loss : 0.3704830285069359\n",
            "\n",
            "Epoch : 18, train loss : 0.09568550274366402\n",
            "Epoch : 18, val loss : 0.30780348625969206\n",
            "\n",
            "Epoch : 19, train loss : 0.07403905410623997\n",
            "Epoch : 19, val loss : 0.3266876851658983\n",
            "\n",
            "Epoch : 20, train loss : 0.05411266917698602\n",
            "Epoch : 20, val loss : 0.33925666154043366\n",
            "0: 0.6867120265960693 / 239: 0.0028732720778078214\n",
            "1: 0.663555920124054 / 239: 0.005649656680837336\n",
            "2: 0.5803810358047485 / 239: 0.008078029215585238\n",
            "3: 0.9866016507148743 / 239: 0.01220606959514538\n",
            "4: 0.4594918191432953 / 239: 0.014128629507878835\n",
            "5: 0.7575160264968872 / 239: 0.017298152631296772\n",
            "6: 0.9391419887542725 / 239: 0.021227617019389963\n",
            "7: 0.6585599184036255 / 239: 0.023983097849530656\n",
            "8: 0.8476312160491943 / 239: 0.027529671975259504\n",
            "9: 0.6488684415817261 / 239: 0.030244602693174676\n",
            "10: 0.7437984943389893 / 239: 0.0333567302845512\n",
            "11: 0.6403036713600159 / 239: 0.036035825143798134\n",
            "12: 0.6904168128967285 / 239: 0.03892459841951666\n",
            "13: 0.6959466338157654 / 239: 0.04183650902125627\n",
            "14: 0.7096841335296631 / 239: 0.0448058987012967\n",
            "15: 0.6949900388717651 / 239: 0.047713806813730865\n",
            "16: 0.6723018288612366 / 239: 0.050526785177167\n",
            "17: 0.723722517490387 / 239: 0.053554912865411294\n",
            "18: 0.6754907965660095 / 239: 0.05638123418995527\n",
            "19: 0.6755509376525879 / 239: 0.059207807150844755\n",
            "20: 0.6802757382392883 / 239: 0.06205414915184596\n",
            "21: 0.6750842332839966 / 239: 0.06487876937479156\n",
            "22: 0.7857884764671326 / 239: 0.06816658726795947\n",
            "23: 0.6714515686035156 / 239: 0.07097600805709553\n",
            "24: 0.6889440417289734 / 239: 0.07385861911035484\n",
            "25: 0.7116116881370544 / 239: 0.07683607387243456\n",
            "26: 0.6737942695617676 / 239: 0.07965529675763024\n",
            "27: 0.7184664607048035 / 239: 0.08266143257647879\n",
            "28: 0.680794358253479 / 239: 0.08550994453569837\n",
            "29: 0.6833325624465942 / 239: 0.08836907659614436\n",
            "30: 0.6638309955596924 / 239: 0.09114661214241923\n",
            "31: 0.7020632028579712 / 239: 0.09408411508324756\n",
            "32: 0.7176609039306641 / 239: 0.09708688037166038\n",
            "33: 0.6067407727241516 / 239: 0.09962554469268194\n",
            "34: 0.6619605422019958 / 239: 0.10239525407428024\n",
            "35: 0.6021851897239685 / 239: 0.10491485737856462\n",
            "36: 0.7039008140563965 / 239: 0.10786004906917716\n",
            "37: 0.7561730146408081 / 239: 0.11102395289612615\n",
            "38: 0.6562549471855164 / 239: 0.11376978949522873\n",
            "39: 0.666907548904419 / 239: 0.11656019764964053\n",
            "40: 0.6569493412971497 / 239: 0.11930893966343613\n",
            "41: 0.8858236074447632 / 239: 0.12301531458998326\n",
            "42: 0.7461617588996887 / 239: 0.12613733031759702\n",
            "43: 0.6305205821990967 / 239: 0.1287754917493924\n",
            "44: 0.6081172227859497 / 239: 0.13131991527569345\n",
            "45: 0.7161512970924377 / 239: 0.13431636421750281\n",
            "46: 0.6422208547592163 / 239: 0.1370034807646125\n",
            "47: 0.6257798075675964 / 239: 0.13962180631928864\n",
            "48: 0.6867791414260864 / 239: 0.14249535921228482\n",
            "49: 0.7277525663375854 / 239: 0.1455403490295969\n",
            "50: 0.6440610289573669 / 239: 0.148235165050339\n",
            "51: 0.604894757270813 / 239: 0.15076610545732985\n",
            "52: 0.7295628786087036 / 239: 0.15381866980297296\n",
            "53: 0.66930091381073 / 239: 0.15661909203649066\n",
            "54: 0.7130768299102783 / 239: 0.15960267709887677\n",
            "55: 0.73758864402771 / 239: 0.1626888220529676\n",
            "56: 0.7008512616157532 / 239: 0.1656212541099373\n",
            "57: 0.6219180226325989 / 239: 0.1682234215686511\n",
            "58: 0.6413587927818298 / 239: 0.17090693116188052\n",
            "59: 0.6688914895057678 / 239: 0.17370564032299252\n",
            "60: 0.6199535131454468 / 239: 0.17629958807673915\n",
            "61: 0.6152023077011108 / 239: 0.17887365630979818\n",
            "62: 0.6727988123893738 / 239: 0.18168871410222234\n",
            "63: 0.6381128430366516 / 239: 0.18435864231576482\n",
            "64: 0.6819673180580139 / 239: 0.18721206205659333\n",
            "65: 0.618510901927948 / 239: 0.1897999737801412\n",
            "66: 0.7048623561859131 / 239: 0.19274918865958018\n",
            "67: 0.6359291076660156 / 239: 0.1954099799050447\n",
            "68: 0.6016702651977539 / 239: 0.1979274287134035\n",
            "69: 0.6066265106201172 / 239: 0.20046561495030774\n",
            "70: 0.6464609503746033 / 239: 0.20317047248325587\n",
            "71: 0.603863000869751 / 239: 0.20569709591785734\n",
            "72: 0.6225619912147522 / 239: 0.20830195780578517\n",
            "73: 0.6566417217254639 / 239: 0.21104941270840216\n",
            "74: 0.7483609914779663 / 239: 0.21418063024596687\n",
            "75: 0.6198644042015076 / 239: 0.21677420515894388\n",
            "76: 0.6093469262123108 / 239: 0.2193237738878657\n",
            "77: 0.5010741949081421 / 239: 0.22142031863643533\n",
            "78: 0.7350000739097595 / 239: 0.22449563275321255\n",
            "79: 0.70001220703125 / 239: 0.22742455412154414\n",
            "80: 0.5221147537231445 / 239: 0.2296091346810552\n",
            "81: 0.6301916837692261 / 239: 0.23224591996879254\n",
            "82: 0.6546880006790161 / 239: 0.23498520030636164\n",
            "83: 0.5225735306739807 / 239: 0.23717170043470465\n",
            "84: 0.6118054986000061 / 239: 0.2397315560773825\n",
            "85: 0.7312636971473694 / 239: 0.2427912368185849\n",
            "86: 0.43415388464927673 / 239: 0.24460778026900026\n",
            "87: 0.48163408041000366 / 239: 0.24662298562636428\n",
            "88: 0.6228715777397156 / 239: 0.24922914285540074\n",
            "89: 0.7585515975952148 / 239: 0.25240299891228446\n",
            "90: 0.4634292423725128 / 239: 0.25434203339919875\n",
            "91: 0.6897801756858826 / 239: 0.257228142920897\n",
            "92: 0.9652640223503113 / 239: 0.2612669045206891\n",
            "93: 1.041856288909912 / 239: 0.26562613585504025\n",
            "94: 0.48945048451423645 / 239: 0.26767404583208726\n",
            "95: 0.6480706334114075 / 239: 0.2703856384405032\n",
            "96: 0.7024756073951721 / 239: 0.27332486692332814\n",
            "97: 0.5726553797721863 / 239: 0.2757209145374377\n",
            "98: 0.666409432888031 / 239: 0.2785092385244169\n",
            "99: 0.4699763059616089 / 239: 0.2804756665828337\n",
            "100: 0.44047072529792786 / 239: 0.2823186403288501\n",
            "101: 0.8518043160438538 / 239: 0.28588267512401266\n",
            "102: 0.6084261536598206 / 239: 0.2884283912481123\n",
            "103: 0.48661383986473083 / 239: 0.2904644324190945\n",
            "104: 0.684945821762085 / 239: 0.29333031451851743\n",
            "105: 0.8241071105003357 / 239: 0.2967784614243766\n",
            "106: 0.6202991008758545 / 239: 0.29937385515189063\n",
            "107: 0.657010555267334 / 239: 0.3021228532910845\n",
            "108: 0.4761680066585541 / 239: 0.30411518804697807\n",
            "109: 0.7346774935722351 / 239: 0.3071891524552301\n",
            "110: 0.5259717702865601 / 239: 0.30938987115935795\n",
            "111: 0.6172849535942078 / 239: 0.3119726533919697\n",
            "112: 0.5539023876190186 / 239: 0.31429023660376476\n",
            "113: 0.39182376861572266 / 239: 0.3159296665979728\n",
            "114: 0.6488693952560425 / 239: 0.31864460130615707\n",
            "115: 0.7123829126358032 / 239: 0.3216252829489847\n",
            "116: 0.9214750528335571 / 239: 0.3254808271031\n",
            "117: 0.5754721760749817 / 239: 0.3278886604757987\n",
            "118: 0.7750463485717773 / 239: 0.33113153222714503\n",
            "119: 0.5790456533432007 / 239: 0.33355431738757685\n",
            "120: 0.6374914646148682 / 239: 0.3362216456914047\n",
            "121: 0.6645968556404114 / 239: 0.33900238567316376\n",
            "122: 0.4238188862800598 / 239: 0.3407756864525782\n",
            "123: 0.5242928266525269 / 239: 0.34296938028794444\n",
            "124: 0.8225753903388977 / 239: 0.3464111183228352\n",
            "125: 0.6065846681594849 / 239: 0.34894912948668244\n",
            "126: 0.5551353096961975 / 239: 0.3512718713682565\n",
            "127: 0.621235191822052 / 239: 0.35387118179429017\n",
            "128: 0.763451337814331 / 239: 0.35706553885627484\n",
            "129: 0.44855424761772156 / 239: 0.35894233487141175\n",
            "130: 0.7034558653831482 / 239: 0.3618856648520944\n",
            "131: 0.4792137145996094 / 239: 0.36389074315585845\n",
            "132: 0.5007075071334839 / 239: 0.3659857536459567\n",
            "133: 0.5573145747184753 / 239: 0.3683176137912223\n",
            "134: 0.7820374369621277 / 239: 0.37158973695842784\n",
            "135: 0.3830380439758301 / 239: 0.37319240659849406\n",
            "136: 0.5824394226074219 / 239: 0.37562939163032427\n",
            "137: 0.4966764748096466 / 239: 0.3777075358763897\n",
            "138: 0.4799356758594513 / 239: 0.37971563493856314\n",
            "139: 0.5551285743713379 / 239: 0.3820383486388616\n",
            "140: 0.5766327381134033 / 239: 0.38445103791967083\n",
            "141: 0.5043042302131653 / 239: 0.3865610974603117\n",
            "142: 0.3442363440990448 / 239: 0.38800141689168843\n",
            "143: 0.5989084243774414 / 239: 0.39050730988071536\n",
            "144: 0.7890803813934326 / 239: 0.3938089014346628\n",
            "145: 0.5047023296356201 / 239: 0.3959206266632637\n",
            "146: 0.2317851036787033 / 239: 0.396890438812547\n",
            "147: 0.41909059882164 / 239: 0.3986439559624283\n",
            "148: 0.6578080654144287 / 239: 0.4013962909641623\n",
            "149: 0.39609798789024353 / 239: 0.40305360472102525\n",
            "150: 0.8285942673683167 / 239: 0.4065205263418132\n",
            "151: 0.46380236744880676 / 239: 0.4084611220215153\n",
            "152: 0.5674458146095276 / 239: 0.4108353722918481\n",
            "153: 0.7631071209907532 / 239: 0.4140282891160772\n",
            "154: 0.47546014189720154 / 239: 0.41601766209472657\n",
            "155: 0.5391089916229248 / 239: 0.4182733482521447\n",
            "156: 0.6128153800964355 / 239: 0.4208374293404143\n",
            "157: 0.781351625919342 / 239: 0.42410668300534876\n",
            "158: 0.513308584690094 / 239: 0.42625441766932404\n",
            "159: 0.5870950222015381 / 239: 0.4287108821973639\n",
            "160: 0.7068901658058167 / 239: 0.43166858163588195\n",
            "161: 0.6728854179382324 / 239: 0.4344840017946193\n",
            "162: 0.38796716928482056 / 239: 0.4361072953899533\n",
            "163: 0.5734357237815857 / 239: 0.4385066080417591\n",
            "164: 0.27898451685905457 / 239: 0.439673907275479\n",
            "165: 0.32951563596725464 / 239: 0.44105263378580223\n",
            "166: 0.35999298095703125 / 239: 0.4425588805680492\n",
            "167: 0.6004811525344849 / 239: 0.4450713540096161\n",
            "168: 0.8899958729743958 / 239: 0.4487951861141115\n",
            "169: 0.4185486137866974 / 239: 0.4505464355441814\n",
            "170: 0.46481722593307495 / 239: 0.45249127749369217\n",
            "171: 0.49342867732048035 / 239: 0.4545558326289243\n",
            "172: 0.7397197484970093 / 239: 0.4576508943381168\n",
            "173: 0.43250173330307007 / 239: 0.45946052502139323\n",
            "174: 0.673340380191803 / 239: 0.46227784878788614\n",
            "175: 0.6245580315589905 / 239: 0.46489106230905347\n",
            "176: 0.3651081323623657 / 239: 0.46641871139843577\n",
            "177: 0.37014833092689514 / 239: 0.46796744918474076\n",
            "178: 0.5416195392608643 / 239: 0.4702336397255812\n",
            "179: 0.5566853284835815 / 239: 0.4725628670414121\n",
            "180: 0.470528781414032 / 239: 0.47453160671260053\n",
            "181: 0.3696081340312958 / 239: 0.47607808426084863\n",
            "182: 0.7379773259162903 / 239: 0.4791658554989921\n",
            "183: 0.4587123990058899 / 239: 0.4810851542396025\n",
            "184: 0.46053823828697205 / 239: 0.48301209247511284\n",
            "185: 0.45251768827438354 / 239: 0.4849054719239596\n",
            "186: 0.5785543322563171 / 239: 0.4873262013476262\n",
            "187: 0.48716557025909424 / 239: 0.48936455101398224\n",
            "188: 0.5042749047279358 / 239: 0.49147448785384806\n",
            "189: 0.4804835617542267 / 239: 0.49348487932562307\n",
            "190: 0.2143721729516983 / 239: 0.49438183402416574\n",
            "191: 0.27257028222084045 / 239: 0.4955222954560521\n",
            "192: 0.5458100438117981 / 239: 0.49780601948873743\n",
            "193: 0.36657342314720154 / 239: 0.49933979950190566\n",
            "194: 0.4394834041595459 / 239: 0.5011786421971339\n",
            "195: 0.18925240635871887 / 239: 0.5019704932697645\n",
            "196: 0.3034006953239441 / 239: 0.5032399522460154\n",
            "197: 0.5684006810188293 / 239: 0.5056181977732908\n",
            "198: 0.835424542427063 / 239: 0.5091136979508099\n",
            "199: 0.19721873104572296 / 239: 0.5099388809258966\n",
            "200: 0.3920538127422333 / 239: 0.5115792734478306\n",
            "201: 0.5987998247146606 / 239: 0.5140847120449631\n",
            "202: 0.9599913954734802 / 239: 0.5181014124444338\n",
            "203: 0.6764891743659973 / 239: 0.5209319110819484\n",
            "204: 0.7095298171043396 / 239: 0.523900655086569\n",
            "205: 0.16330718994140625 / 239: 0.5245839487683323\n",
            "206: 0.5296521782875061 / 239: 0.5268000666691168\n",
            "207: 0.4223736822605133 / 239: 0.5285673205697884\n",
            "208: 0.4364721179008484 / 239: 0.5303935637409216\n",
            "209: 0.6212083697319031 / 239: 0.5329927619406367\n",
            "210: 0.5108182430267334 / 239: 0.5351300767650163\n",
            "211: 0.5409979224205017 / 239: 0.5373936663985749\n",
            "212: 0.5830486416816711 / 239: 0.5398332004641886\n",
            "213: 0.4134851396083832 / 239: 0.5415632638098303\n",
            "214: 0.6615200638771057 / 239: 0.5443311301858852\n",
            "215: 0.39728885889053345 / 239: 0.5459934266665987\n",
            "216: 0.4456733465194702 / 239: 0.547858168702245\n",
            "217: 0.2905580997467041 / 239: 0.5490738929689676\n",
            "218: 0.6741812229156494 / 239: 0.5518947349058532\n",
            "219: 0.3281305432319641 / 239: 0.5532676660490832\n",
            "220: 0.6860488057136536 / 239: 0.5561381631441193\n",
            "221: 0.6543314456939697 / 239: 0.5588759516198263\n",
            "222: 0.31385788321495056 / 239: 0.5601891645203072\n",
            "223: 0.2286825031042099 / 239: 0.5611459950772286\n",
            "224: 0.25993144512176514 / 239: 0.5622335743455206\n",
            "225: 0.4885854721069336 / 239: 0.5642778650237923\n",
            "226: 0.41637498140335083 / 239: 0.5660200197576976\n",
            "227: 0.36771389842033386 / 239: 0.5675585716339333\n",
            "228: 0.9033308625221252 / 239: 0.5713381986737749\n",
            "229: 0.584412157535553 / 239: 0.5737834378266433\n",
            "230: 0.3125082850456238 / 239: 0.5750910038728593\n",
            "231: 0.42966797947883606 / 239: 0.576888777845574\n",
            "232: 0.576987087726593 / 239: 0.579302949760748\n",
            "233: 0.9999363422393799 / 239: 0.5834867838286952\n",
            "234: 0.32241830229759216 / 239: 0.5848358143822416\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4091814571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mmodelBcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVitControl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mfitschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelBschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloaderB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaderB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelBschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaderB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Bschema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mfitcontrol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelBcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloaderB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaderB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1941197939.py\u001b[0m in \u001b[0;36mfitschema\u001b[0;34m(model, trainloader, valloader, name, n_epochs, policy_only)\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m           \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m           \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}